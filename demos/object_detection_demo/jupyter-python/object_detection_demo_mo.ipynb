{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brave-simple",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 3,
        "row": 0,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "# Open Model Zoo Object Detection Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-tampa",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 3,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    },
    "tags": [
     "hide",
     "hidden"
    ]
   },
   "source": [
    "This demo showcases Object Detection with Sync and Async API.\n",
    "\n",
    "Async API usage can improve overall frame-rate of the application, because rather than wait for inference to complete,\n",
    "the app can continue doing things on the host, while accelerator is busy.\n",
    "\n",
    "\n",
    "Other demo objectives are:\n",
    "\n",
    "* Video as input support via OpenCV\\*\n",
    "* Visualization of the resulting bounding boxes\n",
    "\n",
    "See the [Python demo](../python/) for more details about the Async API, and the [Optimization Guide](https://docs.openvinotoolkit.org/latest/_docs_optimization_guide_dldt_optimization_guide.html) for more information on optimizing models.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-addiction",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-warning\" style=\"color:black\"><i>\n",
    "<b>Note: </b>Binder has limited resources. If you run this notebook in Binder, it may suddenly stop. You can reload the page and try a different model, or run the notebook on your own computer.</div>\n",
    "\n",
    "To run this notebook on your own computer:\n",
    "    \n",
    "* clone the Open Model Zoo repository to your computer with <span style=\"font-family: monospace;font-style: normal\">git clone https://github.com/helena-intel/open_model_zoo.git</span>\n",
    "* go to the directory that contains this notebook (demos/object_detection_demo/jupyter-python) and install the requirements in that directory with  <span style=\"font-family: monospace;font-style: normal\">pip install requirements.txt</span>\n",
    "* run <span style=\"font-family: monospace;font-style: normal\">jupyter lab</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-jacob",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 5,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    },
    "tags": [
     "hide",
     "hidden"
    ]
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "billion-berlin",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "hidden": true
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import Layout, fixed, interact, interact_manual\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "from detection_utils import ColorPalette, download_video, draw_detections, get_model, put_highlighted_text\n",
    "\n",
    "open_model_zoo_path =  os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(os.curdir))))\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join(open_model_zoo_path, \"demos\", \"common\", \"python\"))\n",
    "\n",
    "from pipelines import AsyncPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-christianity",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 23,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    },
    "tags": [
     "hide",
     "hidden"
    ]
   },
   "source": [
    "## Settings\n",
    "\n",
    "Set the file and directory paths. The default settings expect that the models are located in `open_model_zoo_models` in your `$HOME` directory, typically `c:\\users\\username` or `/home/username`. You can change this by setting the `base_model_dir` variable to another directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "impressed-disney",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "hidden": true
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "base_model_dir = os.path.expanduser(\"~/open_model_zoo_models\")\n",
    "precision = \"FP16\"\n",
    "num_infer_requests = 3\n",
    "loop = False\n",
    "prob_threshold = 0.5\n",
    "utilization_monitors = \"\"\n",
    "device = \"CPU\"\n",
    "\n",
    "palette = ColorPalette(100)\n",
    "font_scale = 1\n",
    "thickness = 2\n",
    "\n",
    "DOWNLOAD_MODELS=True\n",
    "CONVERT_MODELS=True\n",
    "\n",
    "omz_cache_dir = os.path.expanduser('~')\n",
    "\n",
    "# The settings below are only required if you want to use the Model Converter to convert models to OpenVINO IR format.\n",
    "# You can use this demo with models that are already downloaded in IR format, so use of the model optimizer is optional.\n",
    "\n",
    "# The path to the Model Optimizer is required if models need to be converted to IR. The paths below should work for default installations of \n",
    "# the Intel Distribution of OpenVINO Toolkit https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html\n",
    "# Adjust them if you installed OpenVINO in a different location.\n",
    "# Note that you also need to install the Model Optimizer prerequisites. See the documentation for your OS at \n",
    "# https://docs.openvinotoolkit.org/latest/installation_guides.html\n",
    "if CONVERT_MODELS:\n",
    "    if sys.platform.startswith('win'):\n",
    "        model_optimizer_path = r\"C:\\Program Files (x86)\\intel\\openvino_2021\\deployment_tools\\model_optimizer\\mo.py\"  # Windows\n",
    "    else:\n",
    "        model_optimizer_path = \"/opt/intel/openvino_2021/deployment_tools/model_optimizer/mo.py\"  # Linux/MacOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-uniform",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## Download Models and convert them to IR format\n",
    "\n",
    "The [Model Downloader](https://github.com/openvinotoolkit/open_model_zoo/blob/master/tools/downloader/README.md) downloads models from the Open Model Zoo. Models that are not in OpenVINO IR format are converted to this format by the Model Converter. \n",
    "\n",
    "The [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) models that are compatible with this demo are listed in the file *models.lst* in the same folder as this notebook. By default all these models are downloaded, with the `--list=models.lst` argument for the Model Downloader. You can choose to download a specific model by using `--name=model_name` instead of `--list=models.lst`. If you already have downloaded Open Zoo Models, you can set the `base_model_dir` variable in the *Settings* cell to the folder that contains your models (this should be a folder with subfolders `intel` and `public`) and set `DOWNLOAD_MODELS` to `False`.\n",
    "\n",
    "<div class=\"alert alert-info\" style=\"color:black\"><i>\n",
    "<b>Note: </b>It will take a while to download and convert all the models. </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "minute-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_MODELS:\n",
    "    downloader_command = os.path.join(open_model_zoo_path, \"tools\", \"downloader\", \"downloader.py\")\n",
    "    download_result = subprocess.run(\n",
    "        [\n",
    "            \"python\",\n",
    "            downloader_command,\n",
    "            \"--output_dir\",\n",
    "            base_model_dir,\n",
    "            \"--jobs\",\n",
    "            \"4\",\n",
    "            \"--cache_dir\",\n",
    "            omz_cache_dir,\n",
    "            \"--precision\",\n",
    "            precision,\n",
    "            \"--list\",\n",
    "            \"models.lst\",\n",
    "        ],\n",
    "        shell=False,\n",
    "        check=False,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "    )\n",
    "#     if download_result.returncode == 0:\n",
    "#         print(\n",
    "#             \"Downloading models succeeded. You can set `DOWNLOAD_MODELS=False` to save some time when you run this notebook again.\"\n",
    "#         )\n",
    "#     else:\n",
    "#         print(f\"Downloading models failed. The error message is: {download_result.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "retired-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were some error messages while converting the models. Check `converter_result.stderr` for more details.\n"
     ]
    }
   ],
   "source": [
    "# Convert the models that are not in IR format to IR\n",
    "if CONVERT_MODELS:\n",
    "    converter_command = os.path.join(open_model_zoo_path, \"tools\", \"downloader\", \"converter.py\")\n",
    "    converter_result = subprocess.run(\n",
    "        [\n",
    "            \"python\",\n",
    "            converter_command,\n",
    "            \"--download_dir\",\n",
    "            base_model_dir,\n",
    "            \"--list\",\n",
    "            \"models.lst\",\n",
    "            \"--precisions\",\n",
    "            precision,\n",
    "            \"--mo\",\n",
    "            model_optimizer_path\n",
    "        ],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        capture_output=False,\n",
    "        shell=False,\n",
    "    )\n",
    "    if converter_result.returncode == 0:\n",
    "        print(\"Converting models succeeded.\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"There were some error messages while converting the models. Check `converter_result.stderr` for more details.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-centre",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 25,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    },
    "tags": [
     "hide"
    ]
   },
   "source": [
    "### Get model info\n",
    "\n",
    "The Info Dumper returns information for the Open Model Zoo models. It returns a list of dictionaries with the model name, description, framework, license url, precisions, task type, and the subdirectory for the downloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "chicken-behalf",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "hidden": true
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    },
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "info_command = os.path.join(open_model_zoo_path, \"tools\", \"downloader\", \"info_dumper.py\")\n",
    "info_result = subprocess.run(\n",
    "    [\n",
    "        \"python\",\n",
    "        info_command,\n",
    "        \"--list\",\n",
    "        \"models.lst\",\n",
    "    ],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    capture_output=False,\n",
    "    shell=False,\n",
    "    text=True,\n",
    ")\n",
    "info = json.loads(info_result.stdout)\n",
    "model_names = [model[\"name\"] for model in info if \"intel\" in model[\"subdirectory\"]]\n",
    "model_names = [model[\"name\"] for model in info ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "funny-trader",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 27,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ctdet_coco_dlav0_384',\n",
       " 'description': 'CenterNet object detection model \"ctdet_coco_dlav0_384\" originally trained with PyTorch*. CenterNet models an object as a single point - the center point of its bounding box and uses keypoint estimation to find center points and regresses to object size. For details see paper <https://arxiv.org/abs/1904.07850>, repository <https://github.com/xingyizhou/CenterNet/>.',\n",
       " 'framework': 'pytorch',\n",
       " 'license_url': 'https://raw.githubusercontent.com/xingyizhou/CenterNet/master/LICENSE',\n",
       " 'precisions': ['FP16', 'FP32'],\n",
       " 'subdirectory': 'public/ctdet_coco_dlav0_384',\n",
       " 'task_type': 'detection'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show an example of the information that the Info Dumper returns\n",
    "info[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-debate",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 29,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    },
    "tags": [
     "hide"
    ]
   },
   "source": [
    "The `models.lst` file lists the models that are supported by this demo, sorted by architecture. The model names can contain wildcard. For example, `face-detection-????` means that the demo supports all models with a name that starts with `face-detection-` followed by four digits. \n",
    "\n",
    "We create a `model_architectures` dictionary that maps the model names given by the Info Dumper, to an architecture given by `models.lst`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "established-solution",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "hidden": true
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "model_architectures = {}\n",
    "modellist = open(\"models.lst\").read().splitlines()\n",
    "\n",
    "for line in modellist[1:]:\n",
    "    if line.startswith(\"# For\"):\n",
    "        _, architecture = line.split(\"=\")\n",
    "    else:\n",
    "        model_architectures[line] = architecture\n",
    "        for modelname in model_names:\n",
    "            modelpattern = re.search(line.replace(\"?\", \"[0-9]\"), modelname)\n",
    "            if modelpattern:\n",
    "                model_architectures[modelpattern.group(0)] = architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-recovery",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 31,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    },
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## Create inference functions\n",
    "\n",
    "The `do_inference_on_video` function performs the inference of a model on a specific video. The helper function `process_results` add the time to the result from the pipeline, so that the inference speed can be computed. The function opens the video file given by `input_filename` with OpenCV's `VideoCapture`. It reads the frames sequentially, `jump_frames` frames at a time. If `jump_frames = 1` all frames will be read. By default `jump_frames=10` which means that every tenth frame will be read. While there are new frames, the code:\n",
    "\n",
    "* Checks if there are results from the pipeline. If there are, it records the time, and adds the result to the list of results\n",
    "* Checks if the pipeline is ready. If it is, it sees if there is a new frame. \n",
    "  * If there is a new frame (we have not reached the end of the video), the frame is read, and sent to the detector pipeline for inference. \n",
    "  * If there are no more frames, the video is closed\n",
    "\n",
    "At the end of the function, we wait until the detector is finished, and add the final results to the list of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "consistent-fortune",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "hidden": true
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "jump_frames = 10\n",
    "\n",
    "\n",
    "def do_inference_on_video(detector_pipeline, input_filename):\n",
    "    resultlist = []\n",
    "    next_frame_id = 0\n",
    "    next_frame_id_to_show = 0\n",
    "    overall_start_time = perf_counter()\n",
    "\n",
    "    def process_results(results):\n",
    "        \"\"\"Helper function to add inference time to results\"\"\"\n",
    "        outputs, meta = results\n",
    "        meta[\"end_time\"] = perf_counter()\n",
    "        meta[\"overall_start_time\"] = overall_start_time\n",
    "        return outputs, meta\n",
    "\n",
    "    cap = cv2.VideoCapture(input_filename)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, next_frame_id)\n",
    "        if detector_pipeline.callback_exceptions:\n",
    "            raise detector_pipeline.callback_exceptions[0]\n",
    "\n",
    "        # Process all completed requests\n",
    "        results = detector_pipeline.get_result(next_frame_id_to_show)\n",
    "        if results:\n",
    "            resultlist.append(process_results(results))\n",
    "            next_frame_id_to_show += jump_frames\n",
    "\n",
    "        if detector_pipeline.is_ready():\n",
    "            # Get new image/frame\n",
    "            start_time = perf_counter()\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                cap.release()\n",
    "                continue\n",
    "\n",
    "            # Submit for inference\n",
    "            detector_pipeline.submit_data(frame, next_frame_id, {\"frame\": frame, \"start_time\": start_time})\n",
    "            next_frame_id += jump_frames\n",
    "\n",
    "        else:\n",
    "            # Wait for empty request\n",
    "            detector_pipeline.await_any()\n",
    "        # Process completed requests\n",
    "\n",
    "    detector_pipeline.await_all()\n",
    "\n",
    "    while detector_pipeline.has_completed_request():\n",
    "        results = detector_pipeline.get_result(next_frame_id_to_show)\n",
    "        if results:\n",
    "            resultlist.append(process_results(results))\n",
    "            next_frame_id_to_show += jump_frames\n",
    "\n",
    "    return resultlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "transparent-flesh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'efficientdet-d0-tf',\n",
       "  'description': 'The \"efficientdet-d0-tf\" model is one of the EfficientDet <https://arxiv.org/abs/1911.09070> models  designed to perform object detection. This model was pretrained in TensorFlow*. All the EfficientDet models have been pretrained on the MSCOCO* image database. For details about this family of models, check out the Google AutoML repository <https://github.com/google/automl/tree/master/efficientdet>.',\n",
       "  'framework': 'tf',\n",
       "  'license_url': 'https://raw.githubusercontent.com/google/automl/master/LICENSE',\n",
       "  'precisions': ['FP16', 'FP32'],\n",
       "  'subdirectory': 'public/efficientdet-d0-tf',\n",
       "  'task_type': 'detection'},\n",
       " {'name': 'efficientdet-d1-tf',\n",
       "  'description': 'The \"efficientdet-d1-tf\" model is one of the EfficientDet <https://arxiv.org/abs/1911.09070> models  designed to perform object detection. This model was pretrained in TensorFlow*. All the EfficientDet models have been pretrained on the MSCOCO* image database. For details about this family of models, check out the Google AutoML repository <https://github.com/google/automl/tree/master/efficientdet>.',\n",
       "  'framework': 'tf',\n",
       "  'license_url': 'https://raw.githubusercontent.com/google/automl/master/LICENSE',\n",
       "  'precisions': ['FP16', 'FP32'],\n",
       "  'subdirectory': 'public/efficientdet-d1-tf',\n",
       "  'task_type': 'detection'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item for item in info if item['name'].startswith('effic')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "exposed-opinion",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "hidden": true
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_results_for_model(modelname, num_threads, num_streams, num_requests):\n",
    "    input_filename = get_input_filename()\n",
    "    model_info = [item for item in info if item[\"name\"] == modelname][0]\n",
    "    model_xml = os.path.join(base_model_dir, model_info[\"subdirectory\"], precision, modelname + \".xml\")\n",
    "    resultvideos = []\n",
    "    architecture_type = model_architectures[modelname]\n",
    "    ie = IECore()\n",
    "\n",
    "    model = get_model(ie, model=Path(model_xml), architecture_type=architecture_type, labels=None)\n",
    "    plugin_config = {\n",
    "        \"CPU_THREADS_NUM\": f\"{num_threads}\",\n",
    "        \"CPU_THROUGHPUT_STREAMS\": f\"{num_streams}\",\n",
    "    }\n",
    "    detector_pipeline = AsyncPipeline(ie, model, plugin_config, device=\"CPU\", max_num_requests=num_requests)\n",
    "#     print(\n",
    "#         f\"Starting inference. Model: {modelname}, video: {input_filename},  threads: {num_threads}, streams: {num_streams}, max_num_requests: {num_requests}\"\n",
    "#     )\n",
    "    start_time = perf_counter()\n",
    "    result = do_inference_on_video(detector_pipeline, input_filename)\n",
    "    end_time = perf_counter()\n",
    "\n",
    "    has_landmarks = architecture_type == \"retina\"\n",
    "\n",
    "    resultvideo = make_result_videos(result, has_landmarks)\n",
    "    fps = len(resultvideo) / (end_time - start_time)\n",
    "\n",
    "    return resultvideo, fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-authentication",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 33,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    },
    "tags": [
     "hide"
    ]
   },
   "source": [
    "The `make_result_videos` function takes the output of the `do_inference_on_video` function and returns a list of videoframes with detection boxes drawn on the frame, as well as the fps and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "covered-essex",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "hidden": true
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def make_result_videos(resultlist, has_landmarks):\n",
    "    framelist = list()\n",
    "\n",
    "    for i, (objects, meta) in enumerate(resultlist):\n",
    "        start_time = meta[\"start_time\"]\n",
    "        overall_start_time = meta[\"overall_start_time\"]\n",
    "        end_time = meta[\"end_time\"]\n",
    "        latency = (end_time - start_time) * 1000\n",
    "        fps = (i + 1) / (end_time - overall_start_time)\n",
    "\n",
    "        frame = meta[\"frame\"]\n",
    "        frame = draw_detections(\n",
    "            frame=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),\n",
    "            detections=objects,\n",
    "            palette=palette,\n",
    "            labels=None,\n",
    "            threshold=prob_threshold,\n",
    "            draw_landmarks=has_landmarks,\n",
    "        )\n",
    "        put_highlighted_text(\n",
    "            frame,\n",
    "            \"Latency: {:.1f} ms\".format(latency),\n",
    "            (20, 30),\n",
    "            cv2.FONT_HERSHEY_COMPLEX,\n",
    "            font_scale,\n",
    "            palette[0],\n",
    "            thickness,\n",
    "        )\n",
    "        put_highlighted_text(\n",
    "            frame,\n",
    "            \"FPS: {:.1f}\".format(fps),\n",
    "            (20, 60),\n",
    "            cv2.FONT_HERSHEY_COMPLEX,\n",
    "            font_scale,\n",
    "            palette[0],\n",
    "            thickness,\n",
    "        )\n",
    "\n",
    "        framelist.append(frame)\n",
    "    return framelist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-basis",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 35,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    },
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## Create widgets\n",
    "\n",
    "This demo works with a variety of [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) models and allows you to use your own video.  We create widgets with [IPywidgets](https://github.com/jupyter-widgets/ipywidgets) to easily select a model and choose a video from your PC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-signal",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 37,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "## Download or upload a video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-point",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 39,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### Option 1: Download a sample video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "linear-mustang",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "hidden": true
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "sample_video_base_url = \"https://github.com/intel-iot-devkit/sample-videos/raw/master\"\n",
    "sample_video_filenames = open(\"sample_videos.lst\").read().splitlines()\n",
    "sample_video_list = [(fn[:-4], os.path.join(sample_video_base_url, fn)) for fn in sample_video_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stuffed-change",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 41,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b7904aa40b491eadb6ae5381cf42ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(index=12, options=(('bolt-detection', 'https://github.com/intel-iot-devkit/sample-videos/raw/master/b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_video = widgets.Dropdown(options=sample_video_list, index=12)\n",
    "sample_video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-addition",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 43,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "### Option 2: Upload your own video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "average-institute",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 45,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a7f19901b243bd8d52ff7bc96a8cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uploader = widgets.FileUpload(multiple=False)\n",
    "uploader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-drawing",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 47,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    },
    "tags": [
     "hide"
    ]
   },
   "source": [
    "`get_input_filename` checks if a video was uploaded. If so, it returns the filename of that video. If not, it returns the selected sample video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "brief-tokyo",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "hidden": true
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_input_filename():\n",
    "    \"\"\"If a video is uploaded, returns the filename of that video. If not, returns the selected sample video.\"\"\"\n",
    "    if len(uploader.value) > 0:\n",
    "        uploaded_filename = next(iter(uploader.value))\n",
    "        content = uploader.value[uploaded_filename][\"content\"]\n",
    "        with open(uploaded_filename, \"wb\") as f:\n",
    "            f.write(content)\n",
    "        input_filename = uploaded_filename\n",
    "    else:\n",
    "        input_filename = os.path.basename(sample_video.value)\n",
    "        if not os.path.exists(input_filename):\n",
    "            download_video(sample_video.value)\n",
    "\n",
    "    return input_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-doctrine",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 50,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Detection results of one model, drawn on video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "geological-mentor",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 52,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": false,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1498ac5f34b246a485f84aac04dd4e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model', options=('ctdet_coco_dlav0_384', 'ctdet_coco_dlav0_512', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact_inference = interact_manual.options(manual_name=\"Do inference\")\n",
    "\n",
    "\n",
    "@interact_inference\n",
    "def show_results_on_model(model=model_names, num_threads=(0, 8), num_streams=(0, 8), num_requests=(0, 10)):\n",
    "    resultvideo, fps = get_results_for_model(model, num_threads, num_streams, num_requests)\n",
    "    for item in resultvideo:\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(item)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    print(\n",
    "        f\"Finished inference. Model: {model},  threads: {num_threads}, streams: {num_streams}, max_num_requests: {num_requests}. FPS: {fps:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-sustainability",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 54,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Detection results of multiple models\n",
    "\n",
    "Perform inference on up to four selected models. Show results on three random frames by clicking on the *Show frames* button after inference is complete. Click the button again to show different frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "becoming-portuguese",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "hidden": true
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "select_model_widget = widgets.SelectMultiple(\n",
    "    description=\"Models\",\n",
    "    options=model_names,\n",
    "    index=[2, 5, 8],\n",
    "    rows=32,\n",
    "    layout=Layout(display=\"flex\", flex_flow=\"column\"),\n",
    "    disabled=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "raised-perfume",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 58,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e1f7a973dd459284c5edf86612b8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectMultiple(description='Models', index=(2, 5, 8), layout=Layout(display='flex', flex…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_inference(modelnames=select_model_widget)\n",
    "def multiple(modelnames, num_threads=(0, 8), num_streams=(0, 8), num_requests=(0, 10)):\n",
    "    global resultvideos\n",
    "    resultvideos = []\n",
    "    for i, modelname in enumerate(modelnames):\n",
    "        resultvideo, fps = get_results_for_model(modelname, num_threads, num_streams, num_requests)\n",
    "        resultvideos.append(resultvideo)\n",
    "        print(f\"--- Finished: FPS: {fps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "velvet-reducing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name, FPS\n",
      "ctdet_coco_dlav0_384, 10.26\n",
      "ctdet_coco_dlav0_512, 5.77\n",
      "faceboxes-pytorch, 24.58\n",
      "efficientdet-d0-tf, 22.45\n",
      "efficientdet-d1-tf, 11.15\n",
      "face-detection-0200, 107.70\n",
      "face-detection-0202, 90.25\n",
      "face-detection-0204, 67.31\n",
      "face-detection-0205, 77.97\n",
      "face-detection-0206, 1.16\n",
      "face-detection-adas-0001, 72.89\n",
      "face-detection-retail-0004, 103.41\n",
      "face-detection-retail-0005, 83.56\n",
      "face-detection-retail-0044, 98.04\n",
      "faster-rcnn-resnet101-coco-sparse-60-0001, 0.47\n",
      "pedestrian-and-vehicle-detector-adas-0001, 67.27\n",
      "pedestrian-detection-adas-0002, 73.73\n",
      "pelee-coco, 61.69\n",
      "person-detection-0106, 0.79\n",
      "person-detection-0200, 105.26\n",
      "person-detection-0201, 84.81\n",
      "person-detection-0202, 70.07\n",
      "person-detection-0203, 33.52\n",
      "person-detection-retail-0013, 82.85\n",
      "person-vehicle-bike-detection-2000, 103.00\n",
      "person-vehicle-bike-detection-2001, 87.28\n",
      "person-vehicle-bike-detection-2002, 69.92\n",
      "product-detection-0001, 60.34\n",
      "retinanet-tf, 0.55\n",
      "rfcn-resnet101-coco-tf, 2.57\n",
      "ssd300, 6.99\n",
      "ssd512, 2.44\n",
      "ssd_mobilenet_v1_coco, 92.35\n",
      "ssd_mobilenet_v1_fpn_coco, 3.58\n",
      "ssd_mobilenet_v2_coco, 71.77\n",
      "ssd_resnet50_v1_fpn_coco, 2.51\n",
      "ssd-resnet34-1200-onnx, 1.10\n",
      "ssdlite_mobilenet_v2, 97.47\n",
      "vehicle-detection-0200, 79.13\n",
      "vehicle-detection-0201, 86.01\n",
      "vehicle-detection-0202, 69.31\n",
      "vehicle-detection-adas-0002, 74.65\n",
      "vehicle-license-plate-detection-barrier-0106, 91.01\n",
      "vehicle-license-plate-detection-barrier-0123, 108.22\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Path to the model /home/lena/open_model_zoo_models/public/retinaface-anti-cov/FP16/retinaface-anti-cov.xml doesn't exist or it's a directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-93c2815204fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{modelname}, {fps:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmultiple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-93c2815204fe>\u001b[0m in \u001b[0;36mmultiple\u001b[0;34m(modelnames, num_threads, num_streams, num_requests)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model name, FPS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mresultvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_results_for_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_streams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_requests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m#resultvideos.append(resultvideo)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{modelname}, {fps:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-20b44ff79f18>\u001b[0m in \u001b[0;36mget_results_for_model\u001b[0;34m(modelname, num_threads, num_streams, num_requests)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIECore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_xml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitecture_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marchitecture_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     plugin_config = {\n\u001b[1;32m     11\u001b[0m         \u001b[0;34m\"CPU_THREADS_NUM\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{num_threads}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/open_model_zoo/notebook/detection-async/demos/object_detection_demo/jupyter-python/detection_utils.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(ie, model, architecture_type, labels, keep_aspect_ratio, prob_threshold)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCenterNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0marchitecture_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"retina\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mRetinaFace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No model type or invalid model type (-at) provided: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchitecture_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/open_model_zoo/notebook/detection-async/demos/common/python/models/retinaface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ie, model_path, threshold, mask_threshold)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRetinaFace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Expected 1 input blob\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/open_model_zoo/notebook/detection-async/demos/common/python/models/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ie, model_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reading network from IR...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mie_api.pyx\u001b[0m in \u001b[0;36mopenvino.inference_engine.ie_api.IECore.read_network\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mie_api.pyx\u001b[0m in \u001b[0;36mopenvino.inference_engine.ie_api.IECore.read_network\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Path to the model /home/lena/open_model_zoo_models/public/retinaface-anti-cov/FP16/retinaface-anti-cov.xml doesn't exist or it's a directory"
     ]
    }
   ],
   "source": [
    "\n",
    "def multiple(modelnames, num_threads=(0, 8), num_streams=(0, 8), num_requests=(0, 10)):\n",
    "    global resultvideos\n",
    "    resultvideos = []\n",
    "    print(\"Model name, FPS\")\n",
    "    for i, modelname in enumerate(modelnames):\n",
    "        resultvideo, fps = get_results_for_model(modelname, num_threads, num_streams, num_requests)\n",
    "        #resultvideos.append(resultvideo)\n",
    "        print(f\"{modelname}, {fps:.2f}\")\n",
    "\n",
    "multiple(model_names, 4, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "federal-despite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name, FPS\n",
      "yolo-v1-tiny-tf, 44.52\n",
      "yolo-v2-ava-0001, 13.48\n",
      "yolo-v2-ava-sparse-35-0001, 13.46\n",
      "yolo-v2-ava-sparse-70-0001, 13.43\n",
      "yolo-v2-tf, 6.75\n",
      "yolo-v2-tiny-ava-0001, 43.49\n",
      "yolo-v2-tiny-ava-sparse-30-0001, 43.79\n",
      "yolo-v2-tiny-ava-sparse-60-0001, 44.25\n",
      "yolo-v2-tiny-tf, 53.74\n",
      "yolo-v2-tiny-vehicle-detection-0001, 53.70\n",
      "yolo-v3-tf, 6.35\n",
      "yolo-v3-tiny-tf, 40.07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def multiple(modelnames, num_threads=(0, 8), num_streams=(0, 8), num_requests=(0, 10)):\n",
    "    global resultvideos\n",
    "    resultvideos = []\n",
    "    print(\"Model name, FPS\")\n",
    "    for i, modelname in enumerate(modelnames):\n",
    "        resultvideo, fps = get_results_for_model(modelname, num_threads, num_streams, num_requests)\n",
    "        #resultvideos.append(resultvideo)\n",
    "        print(f\"{modelname}, {fps:.2f}\")\n",
    "\n",
    "multiple(new_model_names, 4, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "valued-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_names = [\n",
    "\n",
    " 'yolo-v1-tiny-tf',\n",
    " 'yolo-v2-ava-0001',\n",
    " 'yolo-v2-ava-sparse-35-0001',\n",
    " 'yolo-v2-ava-sparse-70-0001',\n",
    " 'yolo-v2-tf',\n",
    " 'yolo-v2-tiny-ava-0001',\n",
    " 'yolo-v2-tiny-ava-sparse-30-0001',\n",
    " 'yolo-v2-tiny-ava-sparse-60-0001',\n",
    " 'yolo-v2-tiny-tf',\n",
    " 'yolo-v2-tiny-vehicle-detection-0001',\n",
    " 'yolo-v3-tf',\n",
    " 'yolo-v3-tiny-tf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sexual-accounting",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "default_view": {
        "col": 0,
        "height": 2,
        "row": 60,
        "width": 12
       },
       "grid_default": {
        "col": null,
        "height": 2,
        "hidden": true,
        "row": null,
        "width": 2
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf04faf5dbe45ea9cc7c27261994566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Show frames', style=ButtonStyle()), Output()), _dom_classes=('widget…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual.options(manual_name=\"Show frames\")\n",
    "def show_random_frames():\n",
    "    global resultvideos\n",
    "    try:\n",
    "        fig, ax = plt.subplots(3, len(select_model_widget.value), figsize=(25, 15), squeeze=False)\n",
    "\n",
    "        indices = random.choices(range(len(resultvideos[0])), k=3)\n",
    "        for i in range(len(resultvideos)):\n",
    "            modelname = select_model_widget.value[i]\n",
    "            resultvideo = resultvideos[i]\n",
    "\n",
    "            for j, framenr in enumerate(indices):\n",
    "                ax[j, i].imshow(resultvideo[framenr])\n",
    "                ax[0, i].set_title(modelname)\n",
    "        for a in ax.ravel():\n",
    "            a.axis(\"off\")\n",
    "    except NameError:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "default_view",
    "version": 1,
    "views": {
     "default_view": {
      "cellMargin": 10,
      "defaultCellHeight": 40,
      "maxColumns": 12,
      "name": "active_view",
      "type": "grid"
     },
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 60,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
