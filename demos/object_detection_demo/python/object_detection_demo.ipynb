{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sitting-desktop",
   "metadata": {},
   "source": [
    "# Object Detection Python\\* Demo\n",
    "\n",
    "This demo showcases Object Detection with Sync and Async API.\n",
    "\n",
    "Async API usage can improve overall frame-rate of the application, because rather than wait for inference to complete,\n",
    "the app can continue doing things on the host, while accelerator is busy.\n",
    "Specifically, this demo keeps the number of Infer Requests that you have set using `-nireq` flag.\n",
    "While some of the Infer Requests are processed by IE, the other ones can be filled with new frame data\n",
    "and asynchronously started or the next output can be taken from the Infer Request and displayed.\n",
    "\n",
    "The technique can be generalized to any available parallel slack, for example, doing inference and simultaneously\n",
    "encoding the resulting (previous) frames or running further inference, like some emotion detection on top of\n",
    "the face detection results.\n",
    "There are important performance caveats though, for example the tasks that run in parallel should try to avoid\n",
    "oversubscribing the shared compute resources.\n",
    "For example, if the inference is performed on the FPGA, and the CPU is essentially idle,\n",
    "than it makes sense to do things on the CPU in parallel. But if the inference is performed say on the GPU,\n",
    "than it can take little gain to do the (resulting video) encoding on the same GPU in parallel,\n",
    "because the device is already busy.\n",
    "\n",
    "This and other performance implications and tips for the Async API are covered in the\n",
    "[Optimization Guide](https://docs.openvinotoolkit.org/latest/_docs_optimization_guide_dldt_optimization_guide.html).\n",
    "\n",
    "Other demo objectives are:\n",
    "* Video as input support via OpenCV\\*\n",
    "* Visualization of the resulting bounding boxes and text labels (from the `.labels` file)\n",
    "  or class number (if no file is provided)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "uploader = widgets.FileUpload(multiple=False)\n",
    "uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = 'uploaded_video.mp4'\n",
    "uploaded_filename = next(iter(uploader.value))\n",
    "content =  uploader.value[uploaded_filename]['content']\n",
    "with open(input_filename, 'wb') as f: f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xml = '/home/lena/omz_models/public/yolo-v3-tf/FP16/yolo-v3-tf.xml'\n",
    "#input_filename = '/home/lena/data/supersupershort_10fps.mp4'\n",
    "architecture_type = 'yolo'\n",
    "num_infer_requests = 3\n",
    "loop = False\n",
    "prob_threshold=0.5\n",
    "utilization_monitors=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "import logging\n",
    "import os.path \n",
    "import random\n",
    "import sys\n",
    "from argparse import ArgumentParser, SUPPRESS\n",
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IECore\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "\n",
    "open_model_zoo_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(os.curdir))))\n",
    "base_model_dir = os.curdir  # Models will be downloaded into the `intel` folder in this directory\n",
    "omz_cache_dir = os.path.expanduser(\"~/open_model_zoo_cache\")\n",
    "\n",
    "sys.path.append(os.path.join(open_model_zoo_path, \"demos\", \"common\", \"python\"))\n",
    "\n",
    "from models import *\n",
    "import monitors\n",
    "from pipelines import AsyncPipeline\n",
    "from performance_metrics import PerformanceMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorPalette:\n",
    "    def __init__(self, n, rng=None):\n",
    "        assert n > 0\n",
    "\n",
    "        if rng is None:\n",
    "            rng = random.Random(0xACE)\n",
    "\n",
    "        candidates_num = 100\n",
    "        hsv_colors = [(1.0, 1.0, 1.0)]\n",
    "        for _ in range(1, n):\n",
    "            colors_candidates = [(rng.random(), rng.uniform(0.8, 1.0), rng.uniform(0.5, 1.0))\n",
    "                                 for _ in range(candidates_num)]\n",
    "            min_distances = [self.min_distance(hsv_colors, c) for c in colors_candidates]\n",
    "            arg_max = np.argmax(min_distances)\n",
    "            hsv_colors.append(colors_candidates[arg_max])\n",
    "\n",
    "        self.palette = [self.hsv2rgb(*hsv) for hsv in hsv_colors]\n",
    "\n",
    "    @staticmethod\n",
    "    def dist(c1, c2):\n",
    "        dh = min(abs(c1[0] - c2[0]), 1 - abs(c1[0] - c2[0])) * 2\n",
    "        ds = abs(c1[1] - c2[1])\n",
    "        dv = abs(c1[2] - c2[2])\n",
    "        return dh * dh + ds * ds + dv * dv\n",
    "\n",
    "    @classmethod\n",
    "    def min_distance(cls, colors_set, color_candidate):\n",
    "        distances = [cls.dist(o, color_candidate) for o in colors_set]\n",
    "        return np.min(distances)\n",
    "\n",
    "    @staticmethod\n",
    "    def hsv2rgb(h, s, v):\n",
    "        return tuple(round(c * 255) for c in colorsys.hsv_to_rgb(h, s, v))\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        return self.palette[n % len(self.palette)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.palette)\n",
    "\n",
    "\n",
    "def get_model(ie, model, architecture_type, labels, keep_aspect_ratio=False, prob_threshold=0.5):\n",
    "    if architecture_type == 'ssd':\n",
    "        return SSD(ie, model, labels=labels, keep_aspect_ratio_resize=keep_aspect_ratio)\n",
    "    elif architecture_type == 'yolo':\n",
    "        return YOLO(ie, model, labels=labels,\n",
    "                    threshold=prob_threshold, keep_aspect_ratio=keep_aspect_ratio)\n",
    "    elif architecture_type == 'faceboxes':\n",
    "        return FaceBoxes(ie, model, threshold=prob_threshold)\n",
    "    elif architecture_type == 'centernet':\n",
    "        return CenterNet(ie, model, labels=labels, threshold=prob_threshold)\n",
    "    elif architecture_type == 'retina':\n",
    "        return RetinaFace(ie, model, threshold=prob_threshold)\n",
    "    else:\n",
    "        raise RuntimeError('No model type or invalid model type (-at) provided: {}'.format(architecture_type))\n",
    "\n",
    "\n",
    "def put_highlighted_text(frame, message, position, font_face, font_scale, color, thickness):\n",
    "    cv2.putText(frame, message, position, font_face, font_scale, (255, 255, 255), thickness + 1)  # white border\n",
    "    cv2.putText(frame, message, position, font_face, font_scale, color, thickness)\n",
    "\n",
    "\n",
    "def get_plugin_configs(device, num_streams, num_threads):\n",
    "    config_user_specified = {}\n",
    "\n",
    "    devices_nstreams = {}\n",
    "    if num_streams:\n",
    "        devices_nstreams = {device: num_streams for device in ['CPU', 'GPU'] if device in device} \\\n",
    "            if num_streams.isdigit() \\\n",
    "            else dict(device.split(':', 1) for device in num_streams.split(','))\n",
    "\n",
    "    if 'CPU' in device:\n",
    "        if num_threads is not None:\n",
    "            config_user_specified['CPU_THREADS_NUM'] = str(num_threads)\n",
    "        if 'CPU' in devices_nstreams:\n",
    "            config_user_specified['CPU_THROUGHPUT_STREAMS'] = devices_nstreams['CPU'] \\\n",
    "                if int(devices_nstreams['CPU']) > 0 \\\n",
    "                else 'CPU_THROUGHPUT_AUTO'\n",
    "\n",
    "    if 'GPU' in device:\n",
    "        if 'GPU' in devices_nstreams:\n",
    "            config_user_specified['GPU_THROUGHPUT_STREAMS'] = devices_nstreams['GPU'] \\\n",
    "                if int(devices_nstreams['GPU']) > 0 \\\n",
    "                else 'GPU_THROUGHPUT_AUTO'\n",
    "\n",
    "    return config_user_specified\n",
    "\n",
    "\n",
    "def draw_detections(frame, detections, palette, labels, threshold, draw_landmarks=False):\n",
    "    size = frame.shape[:2]\n",
    "    for detection in detections:\n",
    "        if detection.score > threshold:\n",
    "            xmin = max(int(detection.xmin), 0)\n",
    "            ymin = max(int(detection.ymin), 0)\n",
    "            xmax = min(int(detection.xmax), size[1])\n",
    "            ymax = min(int(detection.ymax), size[0])\n",
    "            class_id = int(detection.id)\n",
    "            color = palette[class_id]\n",
    "            det_label = labels[class_id] if labels and len(labels) >= class_id else '#{}'.format(class_id)\n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "            cv2.putText(frame, '{} {:.1%}'.format(det_label, detection.score),\n",
    "                        (xmin, ymin - 7), cv2.FONT_HERSHEY_COMPLEX, 0.6, color, 1)\n",
    "            if draw_landmarks:\n",
    "                for landmark in detection.landmarks:\n",
    "                    cv2.circle(frame, landmark, 2, (0, 255, 255), 2)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def print_raw_results(size, detections, labels, threshold):\n",
    "    print(' Class ID | Confidence | XMIN | YMIN | XMAX | YMAX ')\n",
    "    for detection in detections:\n",
    "        if detection.score > threshold:\n",
    "            xmin = max(int(detection.xmin), 0)\n",
    "            ymin = max(int(detection.ymin), 0)\n",
    "            xmax = min(int(detection.xmax), size[1])\n",
    "            ymax = min(int(detection.ymax), size[0])\n",
    "            class_id = int(detection.id)\n",
    "            det_label = labels[class_id] if labels and len(labels) >= class_id else '#{}'.format(class_id)\n",
    "            print('{:^9} | {:10f} | {:4} | {:4} | {:4} | {:4} '\n",
    "                     .format(det_label, detection.score, xmin, ymin, xmax, ymax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()\n",
    "\n",
    "model = get_model(ie, model=Path(model_xml), architecture_type=architecture_type, labels=None)\n",
    "plugin_config = get_plugin_configs('CPU','5','3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_landmarks = architecture_type == 'retina'\n",
    "\n",
    "detector_pipeline = AsyncPipeline(ie, model, plugin_config, device='CPU', max_num_requests=5)\n",
    "\n",
    "cap = cv2.VideoCapture(input_filename)\n",
    "\n",
    "next_frame_id = 0\n",
    "next_frame_id_to_show = 0\n",
    "\n",
    "palette = ColorPalette(len(model.labels) if model.labels else 100)\n",
    "\n",
    "metrics = PerformanceMetrics()\n",
    "\n",
    "while cap.isOpened():\n",
    "    if detector_pipeline.callback_exceptions:\n",
    "        raise detector_pipeline.callback_exceptions[0]\n",
    "    # Process all completed requests\n",
    "    results = detector_pipeline.get_result(next_frame_id_to_show)\n",
    "    if results:\n",
    "        objects, frame_meta = results\n",
    "        frame = frame_meta['frame']\n",
    "        start_time = frame_meta['start_time']\n",
    "\n",
    "#         if len(objects):\n",
    "#             print_raw_results(frame.shape[:2], objects, model.labels, prob_threshold)\n",
    "\n",
    "        \n",
    "        frame = draw_detections(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), objects, palette, model.labels, prob_threshold, has_landmarks)\n",
    "        metrics.update(start_time, frame)\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "        \n",
    "            \n",
    "        next_frame_id_to_show += 1\n",
    "\n",
    "    if detector_pipeline.is_ready():\n",
    "        # Get new image/frame\n",
    "        start_time = perf_counter()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            if loop:\n",
    "                cap.open(input_stream)\n",
    "            else:\n",
    "                cap.release()\n",
    "            continue\n",
    "\n",
    "        # Submit for inference\n",
    "        detector_pipeline.submit_data(frame, next_frame_id, {'frame': frame, 'start_time': start_time})\n",
    "        next_frame_id += 1\n",
    "\n",
    "    else:\n",
    "        # Wait for empty request\n",
    "        detector_pipeline.await_any()\n",
    "\n",
    "    \n",
    "\n",
    "detector_pipeline.await_all()\n",
    "\n",
    "# Process completed requests\n",
    "while detector_pipeline.has_completed_request():\n",
    "    results = detector_pipeline.get_result(next_frame_id_to_show)\n",
    "    if results:\n",
    "        objects, frame_meta = results\n",
    "        frame = frame_meta['frame']\n",
    "        start_time = frame_meta['start_time']\n",
    "\n",
    "#         if len(objects):\n",
    "#             print_raw_results(frame.shape[:2], objects, model.labels, prob_threshold)\n",
    "            \n",
    "        frame = draw_detections(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), objects, palette, model.labels, prob_threshold, has_landmarks)\n",
    "        metrics.update(start_time, frame)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        plt.imshow(frame)\n",
    "        plt.show\n",
    "        \n",
    "    next_frame_id_to_show += 1\n",
    "\n",
    "metrics.print_total()\n",
    "#print(presenter.reportMeans())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-baseball",
   "metadata": {},
   "source": [
    "## Demo Output\n",
    "\n",
    "The demo uses matplotlib to display the resulting frame with detections (rendered as bounding boxes and labels, if provided).\n",
    "The demo reports:\n",
    "\n",
    "* **FPS**: average rate of video frame processing (frames per second).\n",
    "* **Latency**: average time required to process one frame (from reading the frame to displaying the results).\n",
    "You can use both of these metrics to measure application-level performance.\n",
    "\n",
    "\n",
    "## See Also\n",
    "* [Using Open Model Zoo demos](../README.md)\n",
    "* [Model Optimizer](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)\n",
    "* [Model Downloader](../../tools/downloader/README.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
